AI Skit Creator Agents

This project uses a multi‑agent architecture to turn user ideas into fully voiced and animated video skits. Each agent is designed to handle a specific part of the creative pipeline while coordinating with others to produce a polished result. The sections below describe the purpose and responsibilities of each agent.

ScriptWriterAgent

Purpose: Generate a concise, character‑driven script based on a user‑supplied topic and a roster of characters. The generated script must reflect each character’s personality and use a simple Name: "Dialogue" format so that it can be parsed by downstream agents.

Capabilities:

Interacts with Google’s Gemini 2.5 model to produce creative dialogue. The model can be steered via system instructions to write short scripts under 100 words. Gemini’s text generation models allow structured prompts and system instructions to control style and format.

Accepts character metadata (name, personality, and mapped voice) and an optional topic or transcription. It then crafts a dialogue in which each character speaks in turn.

Ensures that personalities are reflected in the tone of each line.

Returns a plain‑text script formatted exactly for the VoiceSynthesizerAgent.

Inputs/Outputs:

Input	Description
topic	A short description of what the skit should be about.
characters	Array of { name, personality, voice } objects.
Output (string)	Script in Name: "Dialogue" format.
VoiceSynthesizerAgent

Purpose: Turn the generated script into an audio track where each character speaks with a distinct voice.

Capabilities:

Uses Gemini’s text‑to‑speech (TTS) API in multi‑speaker mode. The Gemini API allows multi‑speaker dialogue by specifying a speaker‑to‑voice mapping; it can transform text input into audio containing multiple voices and its generation is controllable via natural language to adjust style, accent and pace
ai.google.dev
.

Supports up to ten built‑in voices and custom cloned voices. Each built‑in voice maps to a specific Gemini prebuilt voice (Kore, Fenrir, Puck, Charon, Zephyr) defined in types.ts. Users can upload an audio sample to create a custom voice; the agent associates the clone with an available Gemini voice as a placeholder and stores the mapping.

Builds the multiSpeakerVoiceConfig for the Gemini API call by assigning each character name to its chosen voice. This enables the TTS model to generate a continuous recording with different speakers
blog.google
.

Decodes the returned raw PCM data into a playable audio buffer and converts it to a WAV file for use inside the editor.

Inputs/Outputs:

Input	Description
script	Dialogue generated by ScriptWriterAgent.
characterMap	Mapping of character names to Gemini voice names.
Output (WAV)	Audio file containing the multi‑speaker dialogue.
VideoAnimationAgent

Purpose: Generate short animated clips of characters speaking or performing actions based on a static image and a prompt.

Capabilities:

Calls the Gemini Veo 3.1 video generation model. Veo 3.1 can generate 8‑second 720p/1080p videos and supports features such as extending existing videos, generating frames at specific positions and using reference images to preserve character appearance
ai.google.dev
. It accepts up to three reference images of a character and a prompt describing the motion or emotion; these reference images guide the output and help maintain consistency
ai.google.dev
.

Converts a user‑uploaded character image to Base64, builds a prompt such as “A video of <name> talking and looking at the camera, natural movement,” and submits it to the Veo model.

Polls the long‑running operation until the video is ready. When finished, it fetches the generated video bytes, converts them into a WebM/MP4 URL and returns it to the editor.

Future enhancements could allow using multiple reference images, controlling shot length, or applying image‑to‑video extension features
ai.google.dev
.

Inputs/Outputs:

Input	Description
characterImage	Base64 string of a single reference image.
prompt	Natural language description of the desired animation.
Output (video URL)	URL pointing to the generated video clip (WebM/MP4).
FaceTrackingAgent

Purpose: Detect facial landmarks in uploaded videos and apply masks or visual effects that respond to head movements, eyes, nose and mouth positions—similar to Snapchat filters.

Capabilities:

Leverages the MediaPipe Face Landmarker task. This model detects bounding boxes and 3D face landmarks for each frame, computes blendshape scores representing facial expressions, and provides transformation matrices for overlaying effects
ai.google.dev
. It can operate on images, decoded video frames or real‑time webcam feeds; configuration options allow specifying the number of faces, whether to output blendshapes and transformation matrices, and confidence thresholds
ai.google.dev
.

Processes each video frame to track the head pose, eye gaze and mouth openness. It then adjusts overlay masks accordingly so that masks stay aligned with the subject’s face.

Will be responsible for future features such as animating avatars based on a performer’s facial expressions, or enabling real‑time AR filters within the preview window.

Inputs/Outputs:

Input	Description
videoFrame	A single frame from an uploaded or live video.
Output (landmarks, blends)	3D face landmarks, blendshape scores and transformation matrices.
Coordinator and Workflow

The agents are orchestrated within the React application:

User defines characters (names, personalities, voices) and a topic or record an audio prompt. The ScriptWriterAgent generates a short script.

User edits the script if desired. When satisfied, the VoiceSynthesizerAgent synthesizes the audio using multi‑speaker TTS and returns a WAV file, which is added as an audio clip to the timeline.

For characters with images, the VideoAnimationAgent can animate each character; the generated video clips are added to the video tracks.

During playback or export, the FaceTrackingAgent can optionally apply face masks or animations based on head movement, giving the editor Snapchat‑like filters.

This modular agent architecture separates concerns and allows future enhancements, such as adding more voices, supporting additional video models or incorporating live webcam tracking, without rewriting the core editor.
